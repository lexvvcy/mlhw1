{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "5JywoPOO_Oll"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Requirement already satisfied: llama-cpp-python==0.3.4 in ./.venv/lib/python3.9/site-packages (0.3.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.9/site-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.9/site-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.9/site-packages (from llama-cpp-python==0.3.4) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in ./.venv/lib/python3.9/site-packages (from llama-cpp-python==0.3.4) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Requirement already satisfied: googlesearch-python in ./.venv/lib/python3.9/site-packages (1.3.0)\n",
            "Requirement already satisfied: bs4 in ./.venv/lib/python3.9/site-packages (0.0.2)\n",
            "Requirement already satisfied: charset-normalizer in ./.venv/lib/python3.9/site-packages (3.4.1)\n",
            "Requirement already satisfied: requests-html in ./.venv/lib/python3.9/site-packages (0.10.0)\n",
            "Requirement already satisfied: lxml_html_clean in ./.venv/lib/python3.9/site-packages (0.4.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in ./.venv/lib/python3.9/site-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.9/site-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: pyquery in ./.venv/lib/python3.9/site-packages (from requests-html) (2.0.1)\n",
            "Requirement already satisfied: fake-useragent in ./.venv/lib/python3.9/site-packages (from requests-html) (2.0.3)\n",
            "Requirement already satisfied: parse in ./.venv/lib/python3.9/site-packages (from requests-html) (1.20.2)\n",
            "Requirement already satisfied: w3lib in ./.venv/lib/python3.9/site-packages (from requests-html) (2.3.1)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in ./.venv/lib/python3.9/site-packages (from requests-html) (2.0.0)\n",
            "Requirement already satisfied: lxml in ./.venv/lib/python3.9/site-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.9/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.9/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Requirement already satisfied: certifi>=2023 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
            "Requirement already satisfied: websockets<11.0,>=10.0 in ./.venv/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: importlib-resources>=6.0 in ./.venv/lib/python3.9/site-packages (from fake-useragent->requests-html) (6.5.2)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in ./.venv/lib/python3.9/site-packages (from pyquery->requests-html) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kX6SizAt_Olm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!') #because i use mak - -\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "using macbook ... :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.], device='mps:0')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.backends.mps.is_available():\n",
        "    mps_device = torch.device(\"mps\")\n",
        "    x = torch.ones(1, device=mps_device)\n",
        "    print (x)\n",
        "else:\n",
        "    print (\"MPS device not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ScyW45N__Olm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:90]\n",
        "    keyword = f\"{keyword} \\n -site:wikipedia.org \"\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh-TW\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8dmGCARd_Oln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大熱作品——「1989」。她以其寫作歌詞的能力和對情感表達深入人心而聞name。\n",
            "\n",
            "泰勒絲獲得了許多少項獎座，如格萊美、美國唱片業協會（RIAA）等，並且她的專輯銷量超過1億張。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"\"\"請根據以下要求，將輸入的文字萃取出等價的問題：\n",
        "                        - 目標：把輸入的問題改成意義相同，但只用一句話的句子\n",
        "                        - 警告：不允許擅自添加任何詞語，也不允許你回答你認為的答案，例如：\"最近報導中，常提及台灣大學的美食眾多，請問在台灣大學的麥當勞最好吃的是什麼？\"，若回答\"最近在台灣大學的麥當勞最好吃的是什麼？\"則是正確的，但若回答\"台灣大學的麥當勞最好吃的是什麼雞塊？\"，\"台灣大學的麥當勞最好吃的是什麼薯條\"，\"台灣大學的麥當勞最好吃的薯條問題是？\"，\"問題：台灣大學的麥當勞最好吃的薯條是？\"，\"最近報導中，常提及台灣大學的...眾多，請問...麥當勞最好吃的是什麼？\"，等等此類回覆皆為錯誤，因為不允許擅自添加詞語，請直接利用文字中的句子即可\n",
        "                        - 特殊要求：如果問題有特別的輸出格式，請保留該段話，例如：\"最近報導中，常提及台灣大學的美食眾多，請問在台灣大學的麥當勞最好吃的是什麼？請回答漢堡與對應套餐\"，若回答\"最近在台灣大學的麥當勞最好吃的是什麼？請回答漢堡與對應套餐\"則是正確的\n",
        "                        - 結構：保留輸入的問題裡面，靠近問號的地方詢問的問題，以及整體文字中所代表的關鍵字，以及詢問的主題（例如：學校或地名）\n",
        "                        - 範例：“家庭教師動漫中，有如獵人和火影忍者一樣刺激的打鬥畫面，請問家庭教師中專著黑色西裝，並且功力高深的人是誰？” -> 這段話就要輸出 ”家庭教師中專著黑色西裝，並且功力高深的人是誰？“\n",
        "                        - 輸出限制：使用中文時只會使用繁體中文來回問題，不准用簡體中文，非常重要，並且你只會使用原本問題裡的句子回答，如果是英文專有名詞，可以附上。\"\"\",\n",
        "    task_description=\"找出以下段落所問的問題，用一句話呈現：\",\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"\"\"請根據以下要求，將輸入的文字萃取出等價的問題關鍵字：\n",
        "                        - 目標：把輸入的問題改成意義相同，但只用30個字以內呈現的句子，並且皆為問題關鍵字，請特別留意問題時間\n",
        "                        - 特殊要求：如果問題有特別討論的時間，請保留該段話，例如：\"最新的iphone手機出到第幾代？\"，這段話就要輸出 \"最新iphone手機 第幾代\"，如果回答 \"iphone手機 第幾代\"那麼會是不正確的\n",
        "                        - 警告：不允許擅自添加任何詞語，也不允許你回答你認為的答案，例如：\"最近報導中，常提及台灣大學的美食眾多，請問在台灣大學的麥當勞最好吃的是什麼？\"，若回答\"最近在台灣大學的麥當勞最好吃的是什麼？\"則是正確的，但若回答\"台灣大學的麥當勞最好吃的是什麼雞塊？\"，\"台灣大學的麥當勞最好吃的是什麼薯條\"，\"台灣大學的麥當勞最好吃的薯條問題是？\"，\"問題：台灣大學的麥當勞最好吃的薯條是？\"，等等此類回覆皆為錯誤，因為不允許擅自添加詞語，請直接利用文字中的句子即可\n",
        "                        - 結構：保留輸入的問題裡面的結構，特別是靠近問號的地方詢問的問題，以及是整體問題強調的詢問議題（例如「」內的文字很特殊，請保留），如果是很簡單的問題請直接輸出\n",
        "                        - 範例：“家庭教師動漫中，有如獵人和火影忍者一樣刺激的打鬥畫面，請問家庭教師中專著黑色西裝，並且功力高深的人是誰？” -> 這段話就要輸出 ”家庭教師動漫 黑色西裝 功力高深的人“\n",
        "                        - 範例：“台大電機系張耀文教授在2025年報導中指出，台灣大學某課程停修率上升，與「108課綱」相關，請問該課程為何？” -> 這段話就要輸出 ”台大電機系張耀文 台大停修率上升 課程“\n",
        "                        - 範例：“請問200*909=?” -> 這段話就要輸出 ”請問200*909=?“，因為他是很短的問題，並且請不要任意添加不屬於原本問題的詞語\n",
        "                        - 特殊要求：最多只能用30個字回答，總共數個關鍵字，以及特定年份\n",
        "                        - 輸出限制：請注意，不允許擅自添加並非屬於原本文字的詞語，使用中文時只會使用繁體中文來回問題，不准用簡體中文，非常重要，並且你只會使用原本問題裡的句子回答，如果是英文專有名詞，可以附上。\"\"\",\n",
        "    task_description=\"找出以下段落要問的問題，用30個字以內呈現：\",\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是一位只會用五個字以內回答問題的專家，當你看完問題後，你會看到相關資料，看完之後根據對應的問題回答出足夠好的答案，你只需要講出答案，不需要做過多的修飾或解釋，能用一個名詞來符合回答問題是最好的。使用中文時只會使用繁體中文來回問題，可以用數字回答，但有單位要加上單位，專有名詞可以用英文。\",\n",
        "    task_description=\"用最精簡的方式回答問題，比方說一個名詞而已，請記住使用中文時只會使用繁體中文來回問題，不准用簡體中文，殘體中文輸出答案，以下為問題與資料：\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    key = keyword_extraction_agent.inference(question)\n",
        "    q_ext =  question_extraction_agent.inference(question)\n",
        "    #print(key)\n",
        "    #print(q_ext)\n",
        "    results  =  await search(key)\n",
        "    #print(results)\n",
        "    question = f\"以下為問題摘要：{q_ext}\\n以下為資料：{results}\"\n",
        "    question = question[:15000]\n",
        "    return qa_agent.inference(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can try out different questions here.\n",
        "#question='Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data論文中提出的模型是甚麼名字？'\n",
        "#answer = await pipeline(question)\n",
        "#answer = answer.replace('\\n',' ')\n",
        "#print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "plUDRTi_B39S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 光華國小\n",
            "2 750\n",
            "3 史蒂夫·賈伯斯\n",
            "4 TOEFL iBT\n",
            "5 觸地得5分\n",
            "6 卑南族的祖先發源自巴布亞新幾內雅\n",
            "7 李琳山\n"
          ]
        },
        {
          "ename": "CancelledError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[83], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTUDENT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pipeline(question)\n\u001b[1;32m     14\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m, answer)\n",
            "Cell \u001b[0;32mIn[81], line 10\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      7\u001b[0m q_ext \u001b[38;5;241m=\u001b[39m  question_extraction_agent\u001b[38;5;241m.\u001b[39minference(question)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(key)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(q_ext)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results  \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mawait\u001b[39;00m search(key)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(results)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m以下為問題摘要：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_ext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m以下為資料：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[77], line 37\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(keyword, n_results)\u001b[0m\n\u001b[1;32m     35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(_search(keyword, n_results \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh-TW\u001b[39m\u001b[38;5;124m\"\u001b[39m, unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_htmls(results)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Filter out the None values.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m results \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
            "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mget_htmls\u001b[0;34m(urls)\u001b[0m\n\u001b[1;32m     21\u001b[0m session \u001b[38;5;241m=\u001b[39m AsyncHTMLSession()\n\u001b[1;32m     22\u001b[0m tasks \u001b[38;5;241m=\u001b[39m (worker(session, url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"B12901140\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
